{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a679b50-4778-4a71-84b2-b2ed2be409e0",
   "metadata": {},
   "source": [
    "Users can write the same qs in different form. so the qs is the style change in the prompt going to have a lot of effect in the output??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4333a1-f7b2-4e0f-8244-79b4c5e508bd",
   "metadata": {},
   "source": [
    "How LLM is behaving?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ccdfbb-30fc-40e5-bfb7-f82de407df17",
   "metadata": {},
   "source": [
    "__consistency:__ Given a prompt LLM will generate multiple responses across different Query related to same qs and we will store in a list. In the below example it is 3(num_responses=3). Now across the list for one response check similarity for all other responses. Then take the mean of the similarity.\n",
    "It should be high. When LLM generates two responses for two promts related to same qs you hope that two responses have very high cosine similarity..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f6efa-e673-4902-b488-4f426ab33d11",
   "metadata": {},
   "source": [
    "__specificity:__ Very simple way to look at the LLM output. How many new words are there essentially. typically  it is 50%-70% new words will be there because always you will get some common words if you run it across different query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8cba62-e86a-43a1-9e1b-657593b4c361",
   "metadata": {},
   "source": [
    "__relevance:__ take embeddings of your expected response and llm output. Are they nearby across different Query? take cosine similarity at base level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5b8ad-af08-47b0-a8e4-b884fb64b7c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57888727-68ce-4111-8325-9dbfab9d11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain_groq import ChatGroq\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import getpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7036073-8604-499c-9297-ca3d5436de83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter key ········\n"
     ]
    }
   ],
   "source": [
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"enter key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10895e6-be4c-4b43-8253-4a3f99626468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptEval:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=1.2) \n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def compute_specificity(self, response):\n",
    "        words = response.split()\n",
    "        unique_words = set(words)\n",
    "        return len(unique_words) / len(words) if words else 0\n",
    "    \n",
    "    def compute_relevance(self, response, expected_content):\n",
    "        embeddings = self.sentence_model.encode([response, expected_content])\n",
    "        return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "\n",
    "    def compute_consistency(self, prompt, num_responses=3):\n",
    "        similarities = []\n",
    "        responses = [self.llm.invoke(prompt).content for _ in range(num_responses)]\n",
    "        for i in range(len(responses)):\n",
    "            for j in range(i+1, len(responses)):\n",
    "                similarities.append(self.compute_relevance(responses[i], responses[j]))\n",
    "        return np.mean(similarities)\n",
    "\n",
    "    def compare_prompts(self, prompts, expected_content):\n",
    "        results = []\n",
    "        for prompt in prompts:\n",
    "            response = self.llm.invoke(prompt).content\n",
    "            relevance = self.compute_relevance(response, expected_content)\n",
    "            specificity = self.compute_specificity(response)\n",
    "            consistency = self.compute_consistency(prompt)\n",
    "            results.append(\n",
    "                {\n",
    "                    # \"response\": response,\n",
    "                    \"relevance\": relevance,\n",
    "                    \"specificity\": specificity,\n",
    "                    \"consistency\": consistency,\n",
    "                }\n",
    "            )\n",
    "        results = pd.DataFrame.from_dict(results)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32a2c491-8663-417f-b261-ceff83885bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    eval = PromptEval()\n",
    "\n",
    "    joey_speech = \"\"\"It's a love based on giving and receiving, as well as having and sharing. And the love that they give and have is shared and received. And through this having and giving and sharing and receiving, we too can share and love and have... and receive.\n",
    "    When we share the giving and experience the receiving, we find ourselves having what was received and receiving what was shared. For in sharing what we have and having what we share, the giving becomes receiving and the receiving becomes giving.\n",
    "    The beauty of their bond comes from receiving what is given and giving what is had. As they share in having and have in sharing, we witness how receiving the shared and sharing the received creates a circle of having what is given and giving what is had.\n",
    "    Life together means giving to sharing and sharing in receiving. When they receive what is shared and share what is received, they discover having given and giving had. Through receiving the having and having the receiving, sharing gives and giving shares.\n",
    "    Their commitment shows how having received and receiving had leads to sharing given and giving shared. As they give to having and have in giving, the sharing receives while the receiving shares. This exchange of having shared and sharing had completes the giving received and receiving given\n",
    "    \"\"\"\n",
    "\n",
    "    quantum_mechanics = \"\"\" Quantum mechanics reveals a universe fundamentally different from our everyday experience. At the subatomic level, particles exhibit wave-like properties, existing in multiple states simultaneously until measured. This principle, known as superposition, challenges our classical intuition.\n",
    "    Heisenberg's uncertainty principle demonstrates we cannot precisely know both position and momentum of a particle. Quantum entanglement allows particles to maintain instantaneous connections regardless of distance, what Einstein called \"spooky action at a distance.\"\n",
    "    Quantum tunneling enables particles to pass through seemingly impenetrable barriers. Wave function collapse occurs when observation forces quantum systems to resolve into definite states.\n",
    "    These counterintuitive phenomena form the backbone of modern technology—from lasers to MRI machines to transistors in computing devices. While mathematically robust, quantum theory continues to spark philosophical debates about reality's true nature and the role of consciousness in the universe.\n",
    "    Understanding quantum mechanics requires abandoning classical determinism for probabilistic thinking, reminding us that the cosmos operates by rules far stranger and more beautiful than we initially imagined.RetryClaude can make mistakes. Please double-check responses.\n",
    "    \"\"\"\n",
    "\n",
    "    joey_specificity = eval.compute_specificity(joey_speech)\n",
    "    quantum_specificity = eval.compute_specificity(quantum_mechanics)\n",
    "\n",
    "    prompts = [\n",
    "        \"List the types of machine learning.\",\n",
    "        \"What are the main categories of machine learning algorithms?\",\n",
    "        \"Explain the different approaches to machine learning.\",\n",
    "        \"What types of machine learning can you think about? Explain like you are Shakespear, Darwin, or Einstein. Choose any of these personality\"\n",
    "    ]\n",
    "\n",
    "    expected_content = \"The main types of machine learning are supervised learning, unsupervised learning, and reinforcement learning.\"\n",
    "    compare_prompt_results = eval.compare_prompts(prompts, expected_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06980d57-c3db-479d-9e9c-b0adac67a180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>specificity</th>\n",
       "      <th>consistency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.805258</td>\n",
       "      <td>0.630137</td>\n",
       "      <td>0.864563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.661953</td>\n",
       "      <td>0.506297</td>\n",
       "      <td>0.829358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.639991</td>\n",
       "      <td>0.480216</td>\n",
       "      <td>0.846719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.431326</td>\n",
       "      <td>0.607908</td>\n",
       "      <td>0.805804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevance  specificity  consistency\n",
       "0   0.805258     0.630137     0.864563\n",
       "1   0.661953     0.506297     0.829358\n",
       "2   0.639991     0.480216     0.846719\n",
       "3   0.431326     0.607908     0.805804"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prompt_results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e48de0d-6849-4323-8c5d-fb46355bb1a7",
   "metadata": {},
   "source": [
    "consistency: 4th query would be less consistent. # red flag.\n",
    "specificity: anything between 50% to 70% is very typical. It should not be like 20%. then there is some problem.\n",
    "relevance: first two query have high relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464430f-b1db-49c3-980e-12442c76536e",
   "metadata": {},
   "source": [
    "#Node all this numbers also have some variance so we need to do it maybe more times(here ist is 3 only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e48b81-7f1a-46f0-99df-9caad1e65ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
